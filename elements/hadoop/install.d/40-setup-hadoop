#!/bin/bash

if [ "${DIB_DEBUG_TRACE:-0}" -gt 0 ]; then
    set -x
fi
set -eu
set -o pipefail

function install_hadoop_v2 {
    case "$DIB_HADOOP_VERSION" in
        "2.7.1")
            hadoop_native_libs_url="${HADOOP_V2_7_1_NATIVE_LIBS_DOWNLOAD_URL}"
        ;;
        "2.7.3")
            hadoop_native_libs_url="${HADOOP_V2_7_3_NATIVE_LIBS_DOWNLOAD_URL}"
        ;;
        "2.8.0")
            hadoop_native_libs_url="${HADOOP_V2_8_0_NATIVE_LIBS_DOWNLOAD_URL}"
        ;;
        *)
            echo "Invalid DIB_HADOOP_VERSION: $DIB_HADOOP_VERSION"
            exit 1
        ;;
    esac

    package="hadoop-$DIB_HADOOP_VERSION.tar.gz"

    echo "Installing hadoop"

    INSTALL_DIR="/opt"
    HADOOP_HOME="/opt/hadoop"
    mkdir -p "$INSTALL_DIR"
    tar xvf "$tmp_dir/$package" -C "$INSTALL_DIR"
    tar_dir=$(tar --exclude '*/*' -tzf "$tmp_dir/$package")
    if [ "$tar_dir" != "hadoop-$DIB_HADOOP_VERSION/" ]; then
        # NOTE: The HiDB integrated hadoop distribution names the directory in
        # the tar file differently than upstream Apache hadoop.
        mv "$INSTALL_DIR/$tar_dir" "$INSTALL_DIR/hadoop-$DIB_HADOOP_VERSION"
    fi
    ln -s "$INSTALL_DIR/hadoop-$DIB_HADOOP_VERSION" "$HADOOP_HOME"
    chown -R hadoop:hadoop "$INSTALL_DIR/hadoop-$DIB_HADOOP_VERSION"
    chown -R hadoop:hadoop "$HADOOP_HOME"
    rm -r $tmp_dir

    # If specified, inject Hadoop native libraries.
    if [ -n "$hadoop_native_libs_url" -a "$hadoop_native_libs_url" != "none" ]; then
        echo "Inject Hadoop native libs"
        rm -r "$HADOOP_HOME/lib/native"
        wget "$hadoop_native_libs_url"
        native_libs_filename=$(basename "$hadoop_native_libs_url")
        tar xvf "$native_libs_filename" -C "$HADOOP_HOME/lib"
        rm "$native_libs_filename"
    else
        echo "Not injecting Hadoop native libs"
    fi

    # Fake point to old CentOS 6.x location
    ln -s /usr/lib64/libsasl2.so.3 /usr/lib64/libsasl2.so.2

    # Is this causing a NULL pointer exception?
    if false; then
        # The HiBD Hadoop distribution has a dependency on an old version of
        # libsasl2, seen when running:
        # sudo su -lc "hdfs dfsadmin -report" hadoop.
        mkdir sasl2-libs
        pushd sasl2-libs
        wget ftp://195.220.108.108/linux/centos/6.9/os/x86_64/Packages/cyrus-sasl-lib-2.1.23-15.el6_6.2.x86_64.rpm
        rpm2cpio cyrus-sasl-lib-2.1.23-15.el6_6.2.x86_64.rpm | cpio -imdv
        find usr/lib64/ | grep '\.so$' | xargs rm
        rsync -av usr/lib64 /usr
        popd
    fi

    echo "Pre-configuring Hadoop"

    HADOOP_PID_DIR="/var/run/hadoop"

    cat >> /etc/profile.d/hadoop.sh <<EOF
export HADOOP_COMMON_HOME=$HADOOP_HOME
export PATH=\$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HADOOP_HDFS_HOME=\$HADOOP_COMMON_HOME
export HADOOP_YARN_HOME=\$HADOOP_COMMON_HOME
export HADOOP_MAPRED_HOME=\$HADOOP_COMMON_HOME
export HADOOP_PID_DIR=$HADOOP_PID_DIR
export YARN_PID_DIR=$HADOOP_PID_DIR
export HADOOP_MAPRED_PID_DIR=$HADOOP_PID_DIR
EOF

    sed -i -e "s,\#export HADOOP_LOG_DIR=.*,export HADOOP_LOG_DIR=/mnt/log/hadoop/\$USER," \
        -e "s,export HADOOP_SECURE_DN_LOG_DIR=.*,export HADOOP_SECURE_DN_LOG_DIR=/mnt/log/hadoop/hdfs," \
        $HADOOP_HOME/etc/hadoop/hadoop-env.sh
    echo "source $JAVA_RC" >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh

    sed -i -e "s,YARN_LOG_DIR=.*,YARN_LOG_DIR=/mnt/log/hadoop/yarn," \
        $HADOOP_HOME/etc/hadoop/yarn-env.sh
    echo "source $JAVA_RC" >> $HADOOP_HOME/etc/hadoop/yarn-env.sh

    # enable swiftfs
    ln -s ${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-openstack-${DIB_HADOOP_VERSION}.jar ${HADOOP_HOME}/share/hadoop/common/lib/

    if [ "$plugin_type" = spark ]; then
        # Add hadoop path to sudo PATH.
        cat << EOF | sudo tee /etc/sudoers.d/90-hadoop
Defaults    secure_path = /sbin:/bin:/usr/sbin:/usr/bin:/usr/local/bin/:/opt/hadoop/bin:/opt/hadoop/sbin
EOF

        # The spark plugin expects Hadoop configuration in /etc/hadoop as is the
        # case with CDH. Add a symlink so as not to confuse it.
        ln -s "$HADOOP_HOME/etc/hadoop" /etc/hadoop
        chown -h hadoop:hadoop /etc/hadoop
        ln -s /etc/hadoop /etc/hadoop/conf
        chown -h hadoop:hadoop /etc/hadoop/conf

        # The spark plugin expects systemd units to exist for the namenode
        # and datanode services.
        for service in namenode datanode; do
            cat > /usr/lib/systemd/system/hadoop-hdfs-${service}.service << EOF
[Unit]
After=network.target
Wants=network.target

[Service]
Type=oneshot
RemainAfterExit=True
User=hdfs
ExecStartPre=-/usr/bin/install -d -m 0755 -o hdfs -g hdfs /var/run/hadoop-hdfs
ExecStartPre=-/usr/bin/install -d -m 0755 /var/lock/subsys
ExecStart=${HADOOP_HOME}/sbin/hadoop-daemon.sh start ${service}
ExecStop=${HADOOP_HOME}/sbin/hadoop-daemon.sh stop ${service}
EOF
        done
    fi
}

case "$DISTRO_NAME" in
    fedora | ubuntu | rhel | rhel7 | centos | centos7 )
    ;;
    *)
        echo "Unknown distro: $DISTRO_NAME. Exiting."
        exit 1
    ;;
esac

echo "Hadoop setup begins for $DISTRO_NAME"
tmp_dir=/tmp/hadoop

echo "Creating hadoop user & group"
case "$DISTRO_NAME" in
    ubuntu )
        addgroup hadoop
        adduser --ingroup hadoop --disabled-password --gecos GECOS hadoop
        adduser hadoop sudo
        if [ "$plugin_type" = "spark" ]; then
            # An hdfs user is required for spark plugin.
            addgroup hdfs
            adduser --ingroup hdfs hdfs --disabled-password --gecos GECOS hdfs
            adduser hdfs hadoop
        fi
    ;;
    fedora | rhel | rhel7 | centos | centos7 )
        adduser -G adm,wheel hadoop
        if [ "$plugin_type" = "spark" ]; then
            # An hdfs user is required for spark plugin. Adapted from CDH
            # hadoop spec file.
            groupadd -r hdfs
            useradd --comment "Hadoop HDFS" --shell /bin/bash -M -r -g hdfs -G hadoop hdfs
        fi
    ;;
esac

install_hadoop_v2

echo "Applying firstboot script"

RC_SCRIPT_DIR=""
if [ "$DISTRO_NAME" == "ubuntu" ]; then
    # File '/etc/rc.local' may not exist
    if [ -f "/etc/rc.local" ]; then
        mv /etc/rc.local /etc/rc.local.old
    fi
    RC_SCRIPT_DIR="/etc"
else
    # File '/etc/rc.d/rc.local' may not exist
    if [ -f "/etc/rc.d/rc.local" ]; then
        mv /etc/rc.d/rc.local /etc/rc.d/rc.local.old
    fi
    RC_SCRIPT_DIR="/etc/rc.d"

    # In a systemd environment we need to ensure that this script is run after
    # cloud-init as cloud-init installs the default user account which is
    # referenced by the firstboot script.
    if which systemctl >/dev/null 2>&1 ; then
        mkdir -p /etc/systemd/system/cloud-init.service.d
        cat > /etc/systemd/system/cloud-init.service.d/rc-local.conf << EOF
[Unit]
Before=rc-local.service
EOF
    fi
fi

install -D -g root -o root -m 0755 $(dirname $0)/firstboot $RC_SCRIPT_DIR/rc.local
# make sure it is run, be it on SysV, upstart, or systemd
chmod +x $RC_SCRIPT_DIR/rc.local
